{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63144278-9e9a-42e8-9057-6272126c2162",
   "metadata": {},
   "source": [
    "# STAGE 3: PROCESS - Getting to Know the Data\n",
    "## Getting to Know the Data and Define Tasks\n",
    "\n",
    "- What tools are you choosing and why?\n",
    "- Have you ensured your data’s integrity?\n",
    "- What steps have you taken to ensure that your data is clean?\n",
    "- How can you verify that your data is clean and ready to analyze?\n",
    "- Have you documented your cleaning process so you can review and share those results?\n",
    "\n",
    "\n",
    "## Key tasks\n",
    "Now that you know your data is credible and relevant to your problem, you’ll need to clean it so that your analysis will be error-free.\n",
    "- Check the data for errors\n",
    "- Transform the data into the right type\n",
    "- Document the cleaning process\n",
    "- Choose your tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743208e-e55b-45be-8e2d-6ff118723a6a",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "### What tools are you choosing and why?\n",
    "#### Python\n",
    "Since I am familiar with python script, I deicide to use python to carry out my programming tasks. I choose the less challenging tool because I really want to focus on documenting the data analytic process. Jupyter notebook will be my reference document for the future. I hope it will be helpful for people who did not go through the whole certification process. Or because of it, some of you choose to take the course and enjoy the benefit of the what I think very comprehensive and helpful course material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d7648-4241-46ff-b94e-5de63fceec90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Exploration\n",
    "\n",
    "csv files are located at subfolder \"./Data/csv\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe8599-9320-43a0-b2a8-4932919e155d",
   "metadata": {},
   "source": [
    "The first task is to get to know my data. The datatypes are defined for each column.\n",
    "\n",
    "By looking at the first few csv files, this is the interpretations of what they are. \n",
    "So the interpretations are to be further verified against the data in detail. It is likely some clean up and data alignment will be necessary.\n",
    "\n",
    "   * ride_id                       object: each trip is anonymized with an unique id\n",
    "   * rideable_type               category: Divvy bike types\n",
    "   * started_at            datetime64[ns]: Trip start datetime\n",
    "   * ended_at              datetime64[ns]: Trip end datetime\n",
    "   * start_station_name          category: Trip start station name\n",
    "   * start_station_id            category: Trip start station id\n",
    "   * end_station_name            category: Trip end station name\n",
    "   * end_station_id              category: Trip end station id\n",
    "   * start_lat                    float64: latitude of start station\n",
    "   * start_lng                    float64: longitude of start station\n",
    "   * end_lat                      float64: latitude of end station\n",
    "   * end_lng                      float64: longitude of end station\n",
    "   * member_casual               category: rider type (casual or member)\n",
    "\n",
    "Here I define the datatypes for the data columns to make sure they are loaded to dataframes with the right format for future process. \n",
    "keep in mind I performed read_csv prior to this to get the list of header (column) names.\n",
    "from this [article ](https://drawingfromdata.com/pandas/concat/memory/exploding-memory-usage-with-concat-and-categories.html), I learned that it is a good idea to use **categorical** data type when loading known categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb5065-23a7-44ba-84e8-0d127fef5f69",
   "metadata": {},
   "source": [
    "### Checking out one csv file on Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a08ca7-fbf2-46d8-92da-7a70c08e095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str vs categorical\n",
    "dtypes0 = {'ride_id': 'str', 'rideable_type': 'str', 'start_station_name': 'str', 'start_station_id': 'str', 'end_station_name':'str', 'end_station_id': 'str', 'member_casual':'str'}\n",
    "dtypes1 = {'ride_id': 'str', 'rideable_type': 'category', 'start_station_name': 'category', 'start_station_id': 'category', 'end_station_name':'category', 'end_station_id': 'category', 'member_casual':'category'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe1b791-38ac-4b49-a220-9177ef802646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the csv file names\n",
    "import pandas as pd\n",
    "file_list_df = pd.read_csv('file_list_2020.csv', header=None, names= ['filename'])\n",
    "file_list = file_list_df['filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffd220e-124c-47fe-9e54-d3afd552b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read just the first csv file\n",
    "df0= pd.read_csv('./Data/csv/'+file_list[0], parse_dates=['started_at','ended_at'], dtype = dtypes0)\n",
    "df1= pd.read_csv('./Data/csv/'+file_list[0], parse_dates=['started_at','ended_at'], dtype = dtypes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25100e2-40bd-4891-a5cc-8925bbd29a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage for dataframe with \"Str\" as datatype 45.228412  Megabyte\n",
      "memory usage for dataframe with \"category\" as datatype 11.341209  Megabyte\n"
     ]
    }
   ],
   "source": [
    "# Compute memory uses\n",
    "m0 = df0.memory_usage(deep=True).sum()/1e+6 # in Megabyte\n",
    "m1 = df1.memory_usage(deep=True).sum()/1e+6\n",
    "print ('memory usage for dataframe with \"Str\" as datatype', m0 ,' Megabyte')\n",
    "print ('memory usage for dataframe with \"category\" as datatype', m1, ' Megabyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021f7ff7-a559-41e5-b21b-d694bb824bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id                       object\n",
       "rideable_type               category\n",
       "started_at            datetime64[ns]\n",
       "ended_at              datetime64[ns]\n",
       "start_station_name          category\n",
       "start_station_id            category\n",
       "end_station_name            category\n",
       "end_station_id              category\n",
       "start_lat                    float64\n",
       "start_lng                    float64\n",
       "end_lat                      float64\n",
       "end_lng                      float64\n",
       "member_casual               category\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the data types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de5d1e-357a-408f-bf91-3bed1da8956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is proven that by declaring datatypes as category, the memory usage is greatly reduced. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclistic_kernel",
   "language": "python",
   "name": "cyclistic_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
